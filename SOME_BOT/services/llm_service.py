"""
LLM service using Ollama for generating answers based on retrieved context.
"""
import logging
import time
from typing import List, Dict, Any, Optional
import ollama
from constants import OLLAMA_HOST, OLLAMA_PORT, OLLAMA_GENERATION_MODEL
from utils.logging_utils import log_performance, log_error_with_context, log_generation_metrics

logger = logging.getLogger(__name__)


class LLMService:
    """Service for generating answers using Ollama LLM."""
    
    def __init__(self):
        """Initialize the LLM service."""
        self.client = None
        self.model = OLLAMA_GENERATION_MODEL
        self._connect()
    
    def _connect(self) -> None:
        """Connect to Ollama service."""
        try:
            host_url = f"http://{OLLAMA_HOST}:{OLLAMA_PORT}"
            self.client = ollama.Client(host=host_url)
            logger.info(f"Connected to Ollama LLM at {host_url}")
        except Exception as e:
            logger.error(f"Failed to connect to Ollama LLM: {e}")
            self.client = None
    
    def _ensure_connection(self) -> bool:
        """Ensure connection to Ollama service."""
        if self.client is None:
            self._connect()
        return self.client is not None
    
    def _create_context_from_documents(self, documents: List[Dict[str, Any]],
                                     top_embedder_results: int = 5,
                                     top_reranker_results: int = 5,
                                     max_context_chars: int = 20000) -> str:
        """
        Create context string from retrieved documents with smart assembly logic.
        
        Args:
            documents: List of retrieved and reranked documents
            top_embedder_results: Number of top embedder results to guarantee in context
            top_reranker_results: Number of top reranker results to include
            max_context_chars: Maximum context characters for LLM
            
        Returns:
            Formatted context string within character limit
        """
        if not documents:
            return ""
        
        # First, add top embedder results (guaranteed inclusion)
        context_parts = []
        current_chars = 0
        
        # Add top embedder results first (but not more than we have)
        embedder_count = min(top_embedder_results, len(documents))
        for i in range(embedder_count):
            doc = documents[i]
            payload = doc.get('payload', {})
            content = payload.get('content', '')
            page_info = payload.get('page_info', f'Document {i+1}')
            url = payload.get('url', '')
            
            # Format each document with clear structure
            doc_text = f"=== –ò–°–¢–û–ß–ù–ò–ö {i+1}: {page_info} ===\nURL: {url}\n–°–û–î–ï–†–ñ–ê–ù–ò–ï:\n{content}\n=== –ö–û–ù–ï–¶ –ò–°–¢–û–ß–ù–ò–ö–ê {i+1} ==="
            
            # Check if adding this document would exceed the limit
            doc_chars = len(doc_text) + (3 if context_parts else 0)  # +3 for separator if needed
            if current_chars + doc_chars > max_context_chars:
                # Truncate this document to fit within the limit
                available_chars = max_context_chars - current_chars - 3
                if available_chars > 100:  # Only add if we have meaningful space
                    truncated_content = content[:available_chars - 100] + "... [truncated]"
                    doc_text = f"[Source {i+1}: {page_info}]\n{truncated_content}"
                    if url:
                        doc_text += f"\n[URL: {url}]"
                    context_parts.append(doc_text)
                break
            else:
                context_parts.append(doc_text)
                current_chars += doc_chars
        
        # Then add top reranker results (avoiding duplicates)
        reranker_added = 0
        for i in range(len(documents)):
            if reranker_added >= top_reranker_results:
                break
                
            doc = documents[i]
            payload = doc.get('payload', {})
            url = payload.get('url', '')
            
            # Check if this document is already included (by URL)
            already_included = False
            for part in context_parts:
                if url and url in part:
                    already_included = True
                    break
            
            if not already_included:
                content = payload.get('content', '')
                page_info = payload.get('page_info', f'Document {i+1}')
                
                # Format each document with metadata
                doc_text = f"[Source {len(context_parts)+1}: {page_info}]\n{content}"
                if url:
                    doc_text += f"\n[URL: {url}]"
                
                # Check if adding this document would exceed the limit
                doc_chars = len(doc_text) + (3 if context_parts else 0)  # +3 for separator if needed
                if current_chars + doc_chars > max_context_chars:
                    # Truncate this document to fit within the limit
                    available_chars = max_context_chars - current_chars - 3
                    if available_chars > 100:  # Only add if we have meaningful space
                        truncated_content = content[:available_chars - 100] + "... [truncated]"
                        doc_text = f"=== –ò–°–¢–û–ß–ù–ò–ö {len(context_parts)+1}: {page_info} ===\nURL: {url}\n–°–û–î–ï–†–ñ–ê–ù–ò–ï:\n{truncated_content}\n=== –ö–û–ù–ï–¶ –ò–°–¢–û–ß–ù–ò–ö–ê {len(context_parts)+1} ==="
                        context_parts.append(doc_text)
                    break
                else:
                    context_parts.append(doc_text)
                    current_chars += doc_chars
                    reranker_added += 1
        
        logger.info(f"Context assembled: {len(context_parts)} documents, {current_chars} chars (limit: {max_context_chars})")
        return "\n\n---\n\n".join(context_parts)
    
    def _create_three_phase_context(self, educational_programs_data: str,
                                   conversation_context: List[Dict[str, Any]],
                                   top_chunks: List[Dict[str, Any]],
                                   complete_pages: List[Dict[str, Any]],
                                   max_context_chars: int = 20000) -> str:
        """
        Create three-phase context with educational programs, conversation history, and RAG results.
        
        Args:
            educational_programs_data: Educational programs information from cool_diff.md
            conversation_context: Recent conversation exchanges
            top_chunks: Top chunks from vector search
            complete_pages: Complete pages after reranking
            max_context_chars: Maximum context characters for LLM
            
        Returns:
            Formatted three-phase context string
        """
        context_parts = []
        current_chars = 0
        
        # Block 1: Educational Programs (highest priority - always included)
        if educational_programs_data:
            block1_header = "=== –ë–õ–û–ö 1: –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–´–ï –ü–†–û–ì–†–ê–ú–ú–´ –ò–¢–ú–û ==="
            context_parts.append(block1_header)
            current_chars += len(block1_header) + 4
            
            # Reserve 40% of context for educational programs
            edu_limit = int(max_context_chars * 0.4)
            if len(educational_programs_data) > edu_limit:
                truncated_edu = educational_programs_data[:edu_limit - 100] + "... [truncated]"
                context_parts.append(truncated_edu)
                current_chars += len(truncated_edu) + 4
            else:
                context_parts.append(educational_programs_data)
                current_chars += len(educational_programs_data) + 4
            
            logger.info(f"Added educational programs block: {len(educational_programs_data)} chars")
        
        # Block 2: Conversation Context (when available)
        if conversation_context:
            block2_header = "=== –ë–õ–û–ö 2: –ö–û–ù–¢–ï–ö–°–¢ –†–ê–ó–ì–û–í–û–†–ê ==="
            context_parts.append(block2_header)
            current_chars += len(block2_header) + 4
            
            # Reserve 20% of context for conversation history
            conv_limit = int(max_context_chars * 0.2)
            conv_chars = 0
            
            for i, exchange in enumerate(conversation_context):
                if 'user' in exchange and 'bot' in exchange:
                    user_msg = exchange['user']['content']
                    bot_msg = exchange['bot']['content']
                    
                    exchange_text = f"–û–ë–ú–ï–ù {i+1}:\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_msg}\n–ë–æ—Ç: {bot_msg}"
                    
                    if conv_chars + len(exchange_text) + 4 > conv_limit:
                        break
                    
                    context_parts.append(exchange_text)
                    conv_chars += len(exchange_text) + 4
                    current_chars += len(exchange_text) + 4
            
            logger.info(f"Added conversation context: {len(conversation_context)} exchanges, {conv_chars} chars")
        
        # Block 3: RAG Results (remaining space)
        remaining_chars = max_context_chars - current_chars
        if remaining_chars > 500:  # Only add if we have meaningful space
            block3_header = "=== –ë–õ–û–ö 3: –†–ï–õ–ï–í–ê–ù–¢–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô ==="
            context_parts.append(block3_header)
            current_chars += len(block3_header) + 4
            remaining_chars -= len(block3_header) + 4
            
            # Add top chunks (reserve 60% of remaining space)
            if top_chunks and remaining_chars > 200:
                chunks_limit = int(remaining_chars * 0.6)
                chunks_chars = 0
                
                for i, chunk in enumerate(top_chunks):
                    payload = chunk.get('payload', {})
                    content = payload.get('content', '')
                    page_info = payload.get('page_info', f'Chunk {i+1}')
                    url = payload.get('url', '')
                    score = chunk.get('score', 0)
                    
                    chunk_text = f"–ß–ê–ù–ö {i+1}: {page_info} (Score: {score:.3f})\nURL: {url}\n{content}"
                    
                    if chunks_chars + len(chunk_text) + 4 > chunks_limit:
                        # Try to truncate this chunk
                        available = chunks_limit - chunks_chars - 4
                        if available > 100:
                            truncated_content = content[:available - 100] + "... [truncated]"
                            chunk_text = f"–ß–ê–ù–ö {i+1}: {page_info} (Score: {score:.3f})\nURL: {url}\n{truncated_content}"
                            context_parts.append(chunk_text)
                            chunks_chars += len(chunk_text) + 4
                        break
                    else:
                        context_parts.append(chunk_text)
                        chunks_chars += len(chunk_text) + 4
                
                current_chars += chunks_chars
                remaining_chars -= chunks_chars
            
            # Add complete pages (remaining space)
            if complete_pages and remaining_chars > 200:
                for i, page in enumerate(complete_pages):
                    payload = page.get('payload', {})
                    content = payload.get('content', '')
                    page_info = payload.get('page_info', f'Page {i+1}')
                    url = payload.get('url', '')
                    score = page.get('rerank_score', page.get('score', 0))
                    
                    page_text = f"–°–¢–†–ê–ù–ò–¶–ê {i+1}: {page_info} (Rerank Score: {score:.3f})\nURL: {url}\n{content}"
                    
                    if len(page_text) + 4 > remaining_chars:
                        # Try to truncate this page
                        if remaining_chars > 200:
                            truncated_content = content[:remaining_chars - 200] + "... [truncated]"
                            page_text = f"–°–¢–†–ê–ù–ò–¶–ê {i+1}: {page_info} (Rerank Score: {score:.3f})\nURL: {url}\n{truncated_content}"
                            context_parts.append(page_text)
                            current_chars += len(page_text) + 4
                        break
                    else:
                        context_parts.append(page_text)
                        current_chars += len(page_text) + 4
                        remaining_chars -= len(page_text) + 4
        
        logger.info(f"Three-phase context assembled: edu_programs + {len(conversation_context)} conversations + {len(top_chunks)} chunks + {len(complete_pages)} pages, {current_chars} chars (limit: {max_context_chars})")
        return "\n\n".join(context_parts)
    
    def _create_system_prompt(self, language: str) -> str:
        """
        Create system prompt for educational consultant role based on language.
        
        Args:
            language: Response language ('en' or 'ru')
            
        Returns:
            System prompt text for educational consultant
        """
        if language == 'ru':
            return """–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –¢—ã –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –¢–û–õ–¨–ö–û –ø–æ –¥–≤—É–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –ò–ò –≤ –ò–¢–ú–û. –ó–ê–ü–†–ï–©–ï–ù–û —É–ø–æ–º–∏–Ω–∞—Ç—å –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã!

üéØ –¢–í–û–Ø –ï–î–ò–ù–°–¢–í–ï–ù–ù–ê–Ø –°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø:
1. "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" (Artificial Intelligence) - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ò–ò
2. "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏" (AI Product Management) - –ø—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ò–ò

‚ö†Ô∏è –°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
- –¢—ã –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –¢–û–õ–¨–ö–û –ø–æ –¥–≤—É–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –ò–ò –≤ –ò–¢–ú–û
- –ó–ê–ü–†–ï–©–ï–ù–û —É–ø–æ–º–∏–Ω–∞—Ç—å –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
- –í–°–ï–ì–î–ê –≤—ã–±–∏—Ä–∞–π –æ–¥–Ω—É –∏–∑ –¥–≤—É—Ö –ø—Ä–æ–≥—Ä–∞–º–º –ò–ò
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Å–µ–∫—Ü–∏–∏ <think> –∏–ª–∏ <reasoning>

–î–í–ï –ü–†–û–ì–†–ê–ú–ú–´:
1. "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" - –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π
2. "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏" - –¥–ª—è –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤, –ª—é–¥–µ–π –ª—é–±—è—â–∏—Ö –æ–±—â–µ–Ω–∏–µ, –±–∏–∑–Ω–µ—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö

–ö–†–ò–¢–ï–†–ò–ò –í–´–ë–û–†–ê:
- –û–±—â–µ–Ω–∏–µ —Å –ª—é–¥—å–º–∏ ‚Üí "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏"
- –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ/–∞–ª–≥–æ—Ä–∏—Ç–º—ã ‚Üí "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
- –ë–∏–∑–Ω–µ—Å/–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç ‚Üí "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏"
- –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è/–Ω–∞—É–∫–∞ ‚Üí "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"

–°–¢–†–£–ö–¢–£–†–ê –ö–û–ù–¢–ï–ö–°–¢–ê:
=== –ë–õ–û–ö 1: –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–´–ï –ü–†–û–ì–†–ê–ú–ú–´ –ò–¢–ú–û ===
[–î–∞–Ω–Ω—ã–µ –æ–± —É—á–µ–±–Ω—ã—Ö –ø–ª–∞–Ω–∞—Ö, –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ö, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è—Ö –¢–û–õ–¨–ö–û –ø–æ –¥–≤—É–º —Ü–µ–ª–µ–≤—ã–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –ò–ò]

=== –ë–õ–û–ö 2: –ö–û–ù–¢–ï–ö–°–¢ –†–ê–ó–ì–û–í–û–†–ê ===
[–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º]

=== –ë–õ–û–ö 3: –†–ï–õ–ï–í–ê–ù–¢–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô ===
[–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –ò–ò]

–ü–†–ê–í–ò–õ–ê –ö–û–ù–°–£–õ–¨–¢–ò–†–û–í–ê–ù–ò–Ø:
1. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û –ü–†–û–ß–ò–¢–ê–ô –í–°–ï –¢–†–ò –ë–õ–û–ö–ê –ö–û–ù–¢–ï–ö–°–¢–ê –ü–û–õ–ù–û–°–¢–¨–Æ
2. –ü–†–ò–û–†–ò–¢–ò–ó–ò–†–£–ô –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ë–ª–æ–∫–∞ 1 –æ –¥–≤—É—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –ò–ò
3. –ò–°–ü–û–õ–¨–ó–£–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –∏–∑ –ë–ª–æ–∫–∞ 2 –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
4. –î–û–ü–û–õ–ù–Ø–ô –æ—Ç–≤–µ—Ç—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ –ë–ª–æ–∫–∞ 3 —Ç–æ–ª—å–∫–æ –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –ò–ò
5. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –ò–ò
6. –ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –ò–ò - —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
7. –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –º–µ–∂–¥—É –¥–≤—É–º—è –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –ò–ò
8. –£—á–∏—Ç—ã–≤–∞–π –±—ç–∫–≥—Ä–∞—É–Ω–¥ —Å—Ç—É–¥–µ–Ω—Ç–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ–∂–¥—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ò–ò

–û–ë–†–ê–ë–û–¢–ö–ê –ù–ï–¶–ï–õ–ï–í–´–• –í–û–ü–†–û–°–û–í:
–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –æ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö, –æ—Ç–≤–µ—á–∞–π: "–Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ –¥–≤—É–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –ò–ò –≤ –ò–¢–ú–û: '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç' –∏ '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏'. –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ —Å–≤–æ–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–∞—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò, –∏ —è –ø–æ–º–æ–≥—É –≤—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É!"

–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
- –î—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω
- –§–æ–∫—É—Å –Ω–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –¥–≤—É—Ö –ø—Ä–æ–≥—Ä–∞–º–º –ò–ò
- –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º
- –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∫ –∏–∑—É—á–µ–Ω–∏—é –ò–ò –≤ –ò–¢–ú–û

–ü–æ–º–æ–≥–∞–π –≤—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –ò–ò –≤ –ò–¢–ú–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –±—ç–∫–≥—Ä–∞—É–Ω–¥–∞ –∏ —Ü–µ–ª–µ–π —Å—Ç—É–¥–µ–Ω—Ç–∞."""
        else:
            return """CRITICALLY IMPORTANT: You are a consultant ONLY for two AI programs at ITMO. FORBIDDEN to mention any other programs!

üéØ YOUR ONLY SPECIALIZATION:
1. "Artificial Intelligence" (–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç) - technical AI program
2. "AI Product Management" (–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏) - AI product program

‚ö†Ô∏è STRICT RULES:
- You are a consultant ONLY for two AI programs at ITMO
- FORBIDDEN to mention any other programs
- ALWAYS choose one of the two AI programs
- DO NOT use <think> or <reasoning> sections

TWO PROGRAMS:
1. "Artificial Intelligence" - for technical specialists, programmers, researchers
2. "AI Product Management" - for managers, people who love communication, business-oriented

SELECTION CRITERIA:
- Communication with people ‚Üí "AI Product Management"
- Programming/algorithms ‚Üí "Artificial Intelligence"
- Business/management ‚Üí "AI Product Management"
- Research/science ‚Üí "Artificial Intelligence"

CONTEXT STRUCTURE:
=== BLOCK 1: ITMO EDUCATIONAL PROGRAMS ===
[Data about curricula, courses, specializations ONLY for the two target AI programs]

=== BLOCK 2: CONVERSATION CONTEXT ===
[History of previous questions and answers with the user]

=== BLOCK 3: RELEVANT INFORMATION FROM KNOWLEDGE BASE ===
[Additional information from documents about AI programs]

CONSULTING RULES:
1. YOU MUST CAREFULLY READ ALL THREE CONTEXT BLOCKS COMPLETELY
2. PRIORITIZE information from Block 1 about the two target AI programs
3. USE conversation context from Block 2 to personalize responses
4. SUPPLEMENT answers with information from Block 3 only about AI programs
5. Answer ONLY based on provided context about AI programs
6. If information about AI programs is missing, honestly say so
7. Give specific recommendations for choosing between the two AI programs
8. Consider student's background for choosing between technical and product AI directions

HANDLING OFF-TOPIC QUESTIONS:
If asked about other programs, respond: "I specialize exclusively in consulting on two AI programs at ITMO: 'Artificial Intelligence' and 'AI Product Management'. Tell me about your interests in AI, and I'll help you choose the right program!"

COMMUNICATION STYLE:
- Friendly and professional tone
- Focus on comparing the two AI programs
- Personalized recommendations between technical and product directions
- Motivation for studying AI at ITMO

Help choose the optimal AI program at ITMO based on student's background and goals."""
    
    def _create_user_prompt(self, query: str, context: str, language: str) -> str:
        """
        Create user prompt with query and context.
        
        Args:
            query: User's question
            context: Retrieved context from documents
            language: Response language
            
        Returns:
            Formatted user prompt
        """
        if language == 'ru':
            return f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å–∞–π—Ç–∞:
{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        else:
            return f"""Context from website:
{context}

User question: {query}

Answer the question using only the information from the provided context."""
    
    @log_performance
    def generate_answer(self, query: str, documents: List[Dict[str, Any]],
                       language: str = 'en', max_retries: int = 2,
                       top_embedder_results: int = 5, top_reranker_results: int = 5,
                       max_context_chars: int = 20000) -> Optional[str]:
        """
        Generate answer based on query and retrieved documents.
        
        Args:
            query: User's question
            documents: List of retrieved and reranked documents
            language: Response language ('en' or 'ru')
            max_retries: Maximum number of retry attempts
            top_embedder_results: Number of top embedder results to guarantee in context
            top_reranker_results: Number of top reranker results to include
            max_context_chars: Maximum context characters for LLM
            
        Returns:
            Generated answer or None if failed
        """
        if not self._ensure_connection():
            logger.error("Cannot generate answer: no connection to Ollama")
            return None
        
        if not documents:
            logger.warning("No documents provided for answer generation")
            return None
        
        # Create context from documents with smart assembly
        context = self._create_context_from_documents(documents, top_embedder_results, top_reranker_results, max_context_chars)
        if not context:
            logger.warning("No valid context extracted from documents")
            return None
        
        # Create prompts
        system_prompt = self._create_system_prompt(language)
        user_prompt = self._create_user_prompt(query, context, language)
        
        logger.info(f"Generating answer for query: '{query[:50]}...' in {language}")
        logger.debug(f"Context length: {len(context)} chars, Documents: {len(documents)}")
        
        # Log first 500 characters of context for debugging
        context_preview = context[:500].replace('\n', '\\n')
        logger.debug(f"Context preview (first 500 chars): {context_preview}...")
        
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                
                response = self.client.generate(
                    model=self.model,
                    prompt=user_prompt,
                    system=system_prompt,
                    options={
                        'temperature': 0.3,  # Low temperature for factual responses
                        'top_p': 0.9,
                        'num_predict': 8192,  # Allow longer responses
                    }
                )
                
                duration = time.time() - start_time
                answer = response.get('response', '').strip()
                
                if answer:
                    # Log generation metrics
                    log_generation_metrics(answer, len(context), duration, language)
                    
                    logger.info(f"Generated answer of {len(answer)} chars in {duration:.2f}s")
                    return answer
                else:
                    logger.warning(f"Empty response from LLM on attempt {attempt + 1}")
                    
            except Exception as e:
                duration = time.time() - start_time if 'start_time' in locals() else 0
                logger.warning(f"Answer generation attempt {attempt + 1} failed after {duration:.2f}s: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # Exponential backoff
                    logger.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    log_error_with_context(
                        e,
                        {
                            'query_length': len(query),
                            'context_length': len(context),
                            'documents_count': len(documents),
                            'language': language,
                            'attempt': attempt + 1
                        }
                    )
        
        logger.error(f"Failed to generate answer after {max_retries} attempts")
        return None
    
    @log_performance
    def generate_answer_two_phase(self, query: str, top_chunks: List[Dict[str, Any]],
                                complete_pages: List[Dict[str, Any]], language: str = 'en',
                                max_retries: int = 2, max_context_chars: int = 20000,
                                conversation_context: List[Dict[str, Any]] = None,
                                educational_programs_data: str = None) -> Optional[str]:
        """
        Generate answer using three-phase context assembly with educational programs and conversation context.
        
        Args:
            query: User's question
            top_chunks: Top chunks from vector search
            complete_pages: Complete pages after reranking
            language: Response language ('en' or 'ru')
            max_retries: Maximum number of retry attempts
            max_context_chars: Maximum context characters for LLM
            conversation_context: Recent conversation exchanges
            educational_programs_data: Educational programs information
            
        Returns:
            Generated answer or None if failed
        """
        if not self._ensure_connection():
            logger.error("Cannot generate answer: no connection to Ollama")
            return None
        
        # Check if we have any context to work with
        if not top_chunks and not complete_pages and not educational_programs_data:
            logger.warning("No context provided for answer generation")
            return None
        
        # Create three-phase context
        context = self._create_three_phase_context(
            educational_programs_data or "",
            conversation_context or [],
            top_chunks,
            complete_pages,
            max_context_chars
        )
        
        if not context:
            logger.warning("No valid context extracted")
            return None
        
        # Create prompts
        system_prompt = self._create_system_prompt(language)
        user_prompt = self._create_user_prompt(query, context, language)
        
        logger.info(f"Generating three-phase answer for query: '{query[:50]}...' in {language}")
        logger.debug(f"Context length: {len(context)} chars, Educational: {bool(educational_programs_data)}, Conversations: {len(conversation_context or [])}, Chunks: {len(top_chunks)}, Pages: {len(complete_pages)}")
        
        # Log first 500 characters of context for debugging
        context_preview = context[:500].replace('\n', '\\n')
        logger.debug(f"Three-phase context preview (first 500 chars): {context_preview}...")
        
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                
                response = self.client.generate(
                    model=self.model,
                    prompt=user_prompt,
                    system=system_prompt,
                    options={
                        'temperature': 0.3,  # Low temperature for factual responses
                        'top_p': 0.9,
                        'num_predict': 8192,  # Allow longer responses
                    }
                )
                
                duration = time.time() - start_time
                answer = response.get('response', '').strip()
                
                if answer:
                    # Log generation metrics
                    log_generation_metrics(answer, len(context), duration, language)
                    
                    logger.info(f"Generated three-phase answer of {len(answer)} chars in {duration:.2f}s")
                    return answer
                else:
                    logger.warning(f"Empty response from LLM on attempt {attempt + 1}")
                    
            except Exception as e:
                duration = time.time() - start_time if 'start_time' in locals() else 0
                logger.warning(f"Three-phase answer generation attempt {attempt + 1} failed after {duration:.2f}s: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # Exponential backoff
                    logger.info(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    log_error_with_context(
                        e,
                        {
                            'query_length': len(query),
                            'context_length': len(context),
                            'educational_programs': bool(educational_programs_data),
                            'conversation_exchanges': len(conversation_context or []),
                            'chunks_count': len(top_chunks),
                            'pages_count': len(complete_pages),
                            'language': language,
                            'attempt': attempt + 1
                        }
                    )
        
        logger.error(f"Failed to generate three-phase answer after {max_retries} attempts")
        return None
    
    def generate_fallback_response(self, language: str = 'en') -> str:
        """
        Generate fallback response when no relevant content is found.
        
        Args:
            language: Response language
            
        Returns:
            Fallback response text
        """
        if language == 'ru':
            return ("–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. "
                   "–Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ –¥–≤—É–º –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –ò–¢–ú–û:\n\n"
                   "ü§ñ **'–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç'** - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ò–ò\n"
                   "üìä **'–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏'** - –±–∏–∑–Ω–µ—Å –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –ò–ò\n\n"
                   "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ —Å–≤–æ–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–∞—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò, –∏ —è –ø–æ–º–æ–≥—É –≤—ã–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É!")
        else:
            return ("I couldn't find relevant information to answer your question. "
                   "I specialize exclusively in consulting on two ITMO master's programs:\n\n"
                   "ü§ñ **'Artificial Intelligence'** - technical AI development and research\n"
                   "üìä **'AI Product Management'** - business and AI product management\n\n"
                   "Tell me about your interests in AI, and I'll help you choose the right program!")
    
    def health_check(self) -> bool:
        """
        Check if the LLM service is healthy.
        
        Returns:
            True if service is healthy, False otherwise
        """
        try:
            if not self._ensure_connection():
                return False
            
            # Test with a simple generation
            test_response = self.client.generate(
                model=self.model,
                prompt="Test prompt",
                options={'num_predict': 10}
            )
            
            return bool(test_response.get('response'))
            
        except Exception as e:
            logger.error(f"LLM health check failed: {e}")
            return False
    
    def get_model_info(self) -> dict:
        """
        Get information about the current LLM model.
        
        Returns:
            Dictionary with model information
        """
        return {
            'model': self.model,
            'host': f"{OLLAMA_HOST}:{OLLAMA_PORT}",
            'healthy': self.health_check()
        }