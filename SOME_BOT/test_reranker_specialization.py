#!/usr/bin/env python3
"""
Comprehensive test suite for reranker specialization on ITMO AI programs.
Tests the improved reranker prompts, URL-based scoring, and score extraction.
"""

import sys
import os
import logging
from typing import List, Dict, Any
from unittest.mock import Mock, patch

# Add the parent directory to the path to import services
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.reranker_service import RerankerService

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TestRerankerSpecialization:
    """Test suite for reranker AI program specialization."""
    
    def __init__(self):
        """Initialize test suite."""
        self.reranker = RerankerService()
        self.test_results = []
    
    def create_test_document(self, content: str, url: str, page_info: str = "") -> Dict[str, Any]:
        """Create a test document with the expected structure."""
        return {
            'id': f'test_{hash(content)}',
            'score': 0.8,  # Default vector similarity score
            'payload': {
                'content': content,
                'url': url,
                'page_info': page_info or url,
                'filename': url.split('/')[-1] if '/' in url else url
            }
        }
    
    def test_url_based_scoring(self):
        """Test URL-based score boost functionality."""
        logger.info("=== Testing URL-based scoring ===")
        
        test_cases = [
            # High priority URLs (should get 1.8x boost)
            ("https://abit.itmo.ru/program/master/ai", 1.8),
            ("https://abit.itmo.ru/program/master/ai_product", 1.8),
            ("https://ai.itmo.ru/research", 1.8),
            
            # Medium priority URLs (should get 1.3x boost)
            ("https://itmo.ru/ai/courses", 1.3),
            ("https://itmo.ru/artificial-intelligence", 1.3),
            ("https://itmo.ru/machine-learning", 1.3),
            
            # Low priority URLs (should get 0.3x penalty)
            ("https://start.itmo.ru/welcome", 0.3),
            ("https://itmo.ru/contacts", 0.3),
            ("https://student.itmo.ru/ru/scholarship", 0.3),
            ("https://stars.itmo.ru/events", 0.3),
            
            # Neutral URLs (should get 1.0x - no change)
            ("https://itmo.ru/general-info", 1.0),
            ("", 1.0),  # Empty URL
        ]
        
        for url, expected_boost in test_cases:
            actual_boost = self.reranker._get_url_based_score_boost(url)
            status = "‚úÖ PASS" if abs(actual_boost - expected_boost) < 0.01 else "‚ùå FAIL"
            logger.info(f"{status} URL: '{url}' -> Expected: {expected_boost}, Got: {actual_boost}")
            
            self.test_results.append({
                'test': 'url_scoring',
                'url': url,
                'expected': expected_boost,
                'actual': actual_boost,
                'passed': abs(actual_boost - expected_boost) < 0.01
            })
    
    def test_score_extraction(self):
        """Test enhanced score extraction functionality."""
        logger.info("\n=== Testing score extraction ===")
        
        test_cases = [
            # Standard numeric responses
            ("8", 8.0),
            ("7.5", 7.5),
            ("10", 10.0),
            ("0", 0.0),
            
            # Formatted responses
            ("SCORE: 9", 9.0),
            ("Score: 6.5", 6.5),
            ("Rating: 8", 8.0),
            ("Relevance: 7", 7.0),
            
            # Fraction formats
            ("8/10", 8.0),
            ("7.5/10", 7.5),
            ("9 / 10", 9.0),
            ("6 out of 10", 6.0),
            
            # Word-based responses (fallback)
            ("excellent", 9.0),
            ("very good", 8.0),
            ("average", 5.0),
            ("poor", 3.0),
            ("irrelevant", 0.0),
            ("highly relevant", 9.0),
            
            # Edge cases
            ("15", 10.0),  # Should clamp to 10
            ("-5", 0.0),   # Should clamp to 0
            ("invalid", None),  # Should return None
            ("", None),    # Empty string
        ]
        
        for response_text, expected_score in test_cases:
            actual_score = self.reranker._extract_score(response_text)
            
            if expected_score is None:
                status = "‚úÖ PASS" if actual_score is None else "‚ùå FAIL"
            else:
                status = "‚úÖ PASS" if actual_score is not None and abs(actual_score - expected_score) < 0.01 else "‚ùå FAIL"
            
            logger.info(f"{status} Response: '{response_text}' -> Expected: {expected_score}, Got: {actual_score}")
            
            self.test_results.append({
                'test': 'score_extraction',
                'response': response_text,
                'expected': expected_score,
                'actual': actual_score,
                'passed': (expected_score is None and actual_score is None) or 
                         (expected_score is not None and actual_score is not None and abs(actual_score - expected_score) < 0.01)
            })
    
    def test_prompt_generation(self):
        """Test specialized prompt generation."""
        logger.info("\n=== Testing prompt generation ===")
        
        query = "–ö–∞–∫—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –≤—ã–±—Ä–∞—Ç—å –µ—Å–ª–∏ –ª—é–±–ª—é –æ–±—â–∞—Ç—å—Å—è —Å –ª—é–¥—å–º–∏?"
        document = "–ü—Ä–æ–≥—Ä–∞–º–º–∞ '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏' —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–∏–∑–Ω–µ—Å-–∞—Å–ø–µ–∫—Ç–∞—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞."
        url = "https://abit.itmo.ru/program/master/ai_product"
        
        prompt = self.reranker._create_reranking_prompt(query, document, url)
        
        # Check that prompt contains key elements
        required_elements = [
            "ITMO AI Master's programs",
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏",
            "AI Product Management",
            "abit.itmo.ru/program/master/ai",
            "9-10: Documents specifically about these two AI programs",
            "0-3: Irrelevant content",
            "start.itmo.ru",
            "RESPOND WITH ONLY A NUMBER (0-10)"
        ]
        
        for element in required_elements:
            if element in prompt:
                logger.info(f"‚úÖ PASS Found required element: '{element}'")
                self.test_results.append({
                    'test': 'prompt_generation',
                    'element': element,
                    'found': True,
                    'passed': True
                })
            else:
                logger.info(f"‚ùå FAIL Missing required element: '{element}'")
                self.test_results.append({
                    'test': 'prompt_generation',
                    'element': element,
                    'found': False,
                    'passed': False
                })
        
        logger.info(f"\nGenerated prompt length: {len(prompt)} characters")
        logger.info("Sample prompt preview:")
        logger.info(prompt[:500] + "..." if len(prompt) > 500 else prompt)
    
    def test_document_ranking_simulation(self):
        """Test complete document ranking with realistic scenarios."""
        logger.info("\n=== Testing document ranking simulation ===")
        
        # Create test documents representing different types of content
        test_documents = [
            # High priority: Target AI programs
            self.create_test_document(
                "–ú–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç' –≤ –ò–¢–ú–û –≥–æ—Ç–æ–≤–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é, –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º –ò–ò.",
                "https://abit.itmo.ru/program/master/ai"
            ),
            self.create_test_document(
                "–ü—Ä–æ–≥—Ä–∞–º–º–∞ '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏' —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–∏–∑–Ω–µ—Å-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ –∏ –∫–æ–º–º–µ—Ä—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ò–ò-—Ä–µ—à–µ–Ω–∏–π.",
                "https://abit.itmo.ru/program/master/ai_product"
            ),
            
            # Medium priority: AI-related content
            self.create_test_document(
                "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–¢–ú–û –ø—Ä–æ–≤–æ–¥–∏—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
                "https://ai.itmo.ru/research"
            ),
            
            # Low priority: General ITMO content
            self.create_test_document(
                "–ò–¢–ú–û - –≤–µ–¥—É—â–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –†–æ—Å—Å–∏–∏ —Å –±–æ–≥–∞—Ç–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π –∏ —Ç—Ä–∞–¥–∏—Ü–∏—è–º–∏.",
                "https://itmo.ru/about"
            ),
            
            # Very low priority: Irrelevant content
            self.create_test_document(
                "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –Ω–∞ —Å—Ç–∞—Ä—Ç–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ò–¢–ú–û. –ó–¥–µ—Å—å –≤—ã –Ω–∞–π–¥–µ—Ç–µ –æ–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ.",
                "https://start.itmo.ru/welcome"
            ),
            self.create_test_document(
                "–ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: –∞–¥—Ä–µ—Å, —Ç–µ–ª–µ—Ñ–æ–Ω—ã, —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞ –¥–ª—è —Å–≤—è–∑–∏ —Å –ò–¢–ú–û.",
                "https://itmo.ru/contacts"
            ),
            self.create_test_document(
                "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∏–ø–µ–Ω–¥–∏—è—Ö –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –ò–¢–ú–û.",
                "https://student.itmo.ru/ru/scholarship"
            )
        ]
        
        query = "–ö–∞–∫—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –≤—ã–±—Ä–∞—Ç—å –µ—Å–ª–∏ –ª—é–±–ª—é –æ–±—â–∞—Ç—å—Å—è —Å –ª—é–¥—å–º–∏?"
        
        # Mock the LLM response to simulate realistic scoring
        with patch.object(self.reranker, '_get_relevance_score') as mock_score:
            # Define expected base scores based on content relevance
            mock_score.side_effect = [9.0, 8.5, 6.0, 4.0, 1.0, 1.0, 2.0]
            
            # Test chunk-level reranking
            ranked_chunks = self.reranker._rerank_chunks(query, test_documents)
            
            logger.info("Ranked documents (chunks):")
            for i, (doc, score) in enumerate(ranked_chunks):
                url = doc['payload']['url']
                content_preview = doc['payload']['content'][:100] + "..."
                logger.info(f"{i+1}. Score: {score:.2f} | URL: {url}")
                logger.info(f"   Content: {content_preview}")
                
                self.test_results.append({
                    'test': 'document_ranking',
                    'rank': i+1,
                    'score': score,
                    'url': url,
                    'content_preview': content_preview
                })
        
        # Verify expected ranking order
        expected_order = [
            "abit.itmo.ru/program/master/ai",      # Should be #1 (high content + high URL boost)
            "abit.itmo.ru/program/master/ai_product", # Should be #2 (high content + high URL boost)
            "ai.itmo.ru/research",                  # Should be #3 (medium content + high URL boost)
            "itmo.ru/about",                       # Should be #4 (low content + neutral URL)
            "student.itmo.ru/ru/scholarship",      # Should be lower (low content + URL penalty)
            "start.itmo.ru/welcome",               # Should be lower (low content + URL penalty)
            "itmo.ru/contacts"                     # Should be lower (low content + URL penalty)
        ]
        
        actual_order = [doc[0]['payload']['url'] for doc, score in ranked_chunks]
        
        logger.info(f"\nExpected top 3 URLs: {expected_order[:3]}")
        logger.info(f"Actual top 3 URLs: {actual_order[:3]}")
        
        # Check if AI program URLs are in top 2
        top_2_urls = actual_order[:2]
        ai_program_urls = ["abit.itmo.ru/program/master/ai", "abit.itmo.ru/program/master/ai_product"]
        
        ai_programs_in_top_2 = sum(1 for url in ai_program_urls if any(ai_url in url for ai_url in top_2_urls))
        
        if ai_programs_in_top_2 >= 2:
            logger.info("‚úÖ PASS Both AI program URLs are in top 2 positions")
            ranking_success = True
        else:
            logger.info(f"‚ùå FAIL Only {ai_programs_in_top_2} AI program URLs in top 2 positions")
            ranking_success = False
        
        self.test_results.append({
            'test': 'ranking_order',
            'ai_programs_in_top_2': ai_programs_in_top_2,
            'expected': 2,
            'passed': ranking_success
        })
    
    def run_all_tests(self):
        """Run all test suites."""
        logger.info("üöÄ Starting comprehensive reranker specialization tests")
        logger.info("=" * 60)
        
        try:
            self.test_url_based_scoring()
            self.test_score_extraction()
            self.test_prompt_generation()
            self.test_document_ranking_simulation()
            
            # Generate summary
            self.generate_test_summary()
            
        except Exception as e:
            logger.error(f"Test execution failed: {e}")
            raise
    
    def generate_test_summary(self):
        """Generate and display test summary."""
        logger.info("\n" + "=" * 60)
        logger.info("üìä TEST SUMMARY")
        logger.info("=" * 60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['passed'])
        failed_tests = total_tests - passed_tests
        
        logger.info(f"Total tests: {total_tests}")
        logger.info(f"Passed: {passed_tests} ‚úÖ")
        logger.info(f"Failed: {failed_tests} ‚ùå")
        logger.info(f"Success rate: {(passed_tests/total_tests)*100:.1f}%")
        
        # Group results by test type
        test_types = {}
        for result in self.test_results:
            test_type = result['test']
            if test_type not in test_types:
                test_types[test_type] = {'total': 0, 'passed': 0}
            test_types[test_type]['total'] += 1
            if result['passed']:
                test_types[test_type]['passed'] += 1
        
        logger.info("\nResults by test type:")
        for test_type, stats in test_types.items():
            success_rate = (stats['passed'] / stats['total']) * 100
            logger.info(f"  {test_type}: {stats['passed']}/{stats['total']} ({success_rate:.1f}%)")
        
        # Show failed tests
        failed_results = [r for r in self.test_results if not r['passed']]
        if failed_results:
            logger.info(f"\n‚ùå Failed tests ({len(failed_results)}):")
            for result in failed_results:
                logger.info(f"  - {result['test']}: {result}")
        
        logger.info("\nüéØ EXPECTED BEHAVIOR VERIFICATION:")
        logger.info("‚úÖ Specialized prompts prioritize AI program content")
        logger.info("‚úÖ URL-based scoring boosts target program pages")
        logger.info("‚úÖ Enhanced score extraction handles various response formats")
        logger.info("‚úÖ Complete ranking pipeline prioritizes AI programs correctly")
        
        if failed_tests == 0:
            logger.info("\nüéâ ALL TESTS PASSED! Reranker specialization is working correctly.")
        else:
            logger.info(f"\n‚ö†Ô∏è  {failed_tests} tests failed. Review the issues above.")


def main():
    """Main test execution function."""
    try:
        # Initialize and run tests
        test_suite = TestRerankerSpecialization()
        test_suite.run_all_tests()
        
        return len([r for r in test_suite.test_results if not r['passed']]) == 0
        
    except Exception as e:
        logger.error(f"Test suite execution failed: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)